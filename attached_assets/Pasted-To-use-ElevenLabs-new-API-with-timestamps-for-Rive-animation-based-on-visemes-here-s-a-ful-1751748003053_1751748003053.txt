To use **ElevenLabs’ new API with timestamps** for **Rive animation based on visemes**, here’s a full breakdown of how to go from **LLM response → ElevenLabs audio with word timings → viseme mapping → Rive animation**.

---

## ✅ Step 1: Get Word Timestamps from ElevenLabs API

ElevenLabs has added **optional timestamp output** when using their `v1/text-to-speech/{voice_id}/stream` endpoint with the query parameter:

```
?output_format=audio/mpeg&timestamp_start=true
```

### API Request (example in Node.js / Python):

```http
POST https://api.elevenlabs.io/v1/text-to-speech/{voice_id}/stream?output_format=audio/mpeg&timestamp_start=true
Authorization: Bearer YOUR_API_KEY
Content-Type: application/json

{
  "text": "Hello there, how are you doing today?",
  "model_id": "eleven_monolingual_v1",
  "voice_settings": {
    "stability": 0.5,
    "similarity_boost": 0.75
  }
}
```

### Response:

* The response is a **multipart stream** that contains:

  1. The **audio file** (MPEG, PCM, etc.)
  2. A **JSON metadata block** with **timestamped characters and words**

Example timestamp structure:

```json
{
  "timestamps": [
    { "char": "H", "start": 0.012, "end": 0.045 },
    { "char": "e", "start": 0.045, "end": 0.072 },
    ...
    { "word": "Hello", "start": 0.012, "end": 0.265 },
    { "word": "there", "start": 0.266, "end": 0.512 },
    ...
  ]
}
```

---

## ✅ Step 2: Map Word Timings to Phonemes/Visemes

**ElevenLabs does not return phonemes or visemes.** But you can now do this:

### Option 1: Use a phoneme dictionary (like CMUdict or espeak)

* Convert each word into phonemes:

  * “hello” → HH AH0 L OW1
* Use the total duration of the word from the ElevenLabs API
* Split it proportionally across the phonemes

Example (for “Hello” with 0.253s duration):

```
"Hello": 0.012 - 0.265
Phonemes:
- HH: 0.012 - 0.085
- AH0: 0.085 - 0.165
- L: 0.165 - 0.215
- OW1: 0.215 - 0.265
```

### Option 2: Use Gentle or MFA forced aligner

* Feed ElevenLabs audio and text into a tool like:

  * [Gentle](https://github.com/lowerquality/gentle)
  * [Montreal Forced Aligner](https://montreal-forced-aligner.readthedocs.io/)
* It gives you phoneme-level timing:

```json
[
  { "phoneme": "HH", "start": 0.012, "end": 0.085 },
  { "phoneme": "AH", "start": 0.085, "end": 0.165 },
  ...
]
```

---

## ✅ Step 3: Map Phonemes to Visemes

Use a **standard phoneme-to-viseme map** (like the Oculus or Rive set). Example:

| Phoneme | Viseme |
| ------- | ------ |
| AH, AA  | A      |
| F, V    | F      |
| M, B, P | M      |
| CH, JH  | CH     |
| etc.    |        |

Use this to create a sequence:

```json
[
  { "viseme": "M", "start": 0.012, "end": 0.045 },
  { "viseme": "A", "start": 0.045, "end": 0.165 },
  ...
]
```

---

## ✅ Step 4: Drive Rive Animation

In your web app:

1. Load Rive animation with viseme states or blendshapes (e.g., “A”, “M”, “F”).
2. Sync playback of ElevenLabs audio.
3. At each viseme's `start` time, trigger the corresponding mouth shape using:

   ```js
   riveInstance.stateMachineInputs["Viseme"].value = "A";
   ```

Optional:

* Crossfade between visemes
* Blend with “emotion” state (e.g., “happy\_talking”)

---

## ✅ Optional Emotion Tag

If your LLM outputs an emotion tag (e.g., “happy”, “sad”), you can:

* Choose an ElevenLabs voice with appropriate tone
* OR use voice settings like:

  ```json
  "voice_settings": {
    "stability": 0.1,  // more variation
    "similarity_boost": 0.9
  }
  ```
* AND switch Rive facial animation state:

  ```js
  riveInstance.stateMachineInputs["Emotion"].value = "happy";
  ```

---

## Summary

| Step              | Tool/API                       | Output                            |
| ----------------- | ------------------------------ | --------------------------------- |
| TTS + Timestamps  | ElevenLabs API                 | Audio + word/char timestamps      |
| Phoneme alignment | CMUdict (approx) or Gentle/MFA | Phoneme timing per word           |
| Viseme mapping    | Lookup table                   | Viseme + timestamp data           |
| Animation         | Rive (JS API)                  | Lip-sync driven by viseme timings |

---

Would you like:

* A sample `phoneme → viseme` table?
* Help running Gentle or another aligner on ElevenLabs output?
* Code to sync this in a React or HTML/JS app?

Let me know how deep you want to go and I’ll help implement it.
